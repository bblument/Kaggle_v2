# Kaggle_v2
# The language of this code is R. The code follows the steps of the tutorial.setwd("C:/Users/DBBL103/Desktop/Marketing/Data_Science_Axa-master")train <- read.csv("C:/Users/DBBL103/Desktop/Marketing/Data_Science_Axa-master/train.csv")test <- read.csv("C:/Users/DBBL103/Desktop/Marketing/Data_Science_Axa-master/test.csv")str(train)train$Survivedtable(train$Survived)prop.table(table(train$Survived))test$Survived <- rep(0, 418)submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)write.csv(submit, file = "theyallperish.csv", row.names = FALSE)summary(train$Sex)#prop.table(table(train$Sex, train$Survived))prop.table(table(train$Sex, train$Survived),1)test$Survived[test$Sex == 'female'] <- 1summary(train$Age)#There are NA's due to missing values --> we could put them to the average#Creation of a new variable child because proportion on a quantitative variable would be stupid (nothing has to be done here for the NA's because they fail the condition and they are > 18 so right values (0) are put in place)train$Child <- 0train$Child[train$Age < 18] <- 1aggregate(Survived ~ Child + Sex, data=train, FUN=sum)aggregate(Survived ~ Child + Sex, data=train, FUN=length)aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})train$Fare2 <- '30+'train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'train$Fare2[train$Fare < 10] <- '<10'aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})test$Survived <- 0test$Survived[test$Sex == 'female'] <- 1test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0#Now we will automate that with decision trees#For regression trees : library(rpart)fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,               data=train,               method="class")# si on veut une survie qui varie continuellement entre 0 et 1 : fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,               data=train,               method="anova")plot(fit)text(fit)install.packages('rattle')install.packages('rpart.plot')install.packages('RColorBrewer')library(rattle)library(rpart.plot)library(RColorBrewer)Prediction <- predict(fit, test, type = "class")submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)write.csv(submit, file = "myfirstdtree.csv", row.names = FALSE)# Ci-dessous : pas top pour l'overfitting mais c'est une façon de contrôler et de prolonger l'arbrefit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,               data=train,               method="class",                control=rpart.control(minsplit=2, cp=0))fancyRpartPlot(fit)# Lancer les commandes ci-dessous pour pouvoir éliminer des noeuds inutiles en cliquant dessus dans le plot et sauvegarder le tree dans new.fit en cliquant sur "quit" toujours dans le plotnew.fit <- prp(fit,snip=TRUE)$objfancyRpartPlot(new.fit)#Feature Engineering :train$Name[1]# faire des modifs dans les 2 databases : An easy way to perform the same processes on both datasets at the same time is to merge them. In R we can use rbind so long as both dataframes have the same columns as each othertest$Survived <- NAcombi <- rbind(train, test)# pour transformer les factors en string : combi$Name <- as.character(combi$Name)combi$Name[1]# pour séparer les caractères (pour une ligne):strsplit(combi$Name[1], split='[,.]')strsplit(combi$Name[1], split='[,.]')[[1]] #nécessaire de passer par làstrsplit(combi$Name[1], split='[,.]')[[1]][[2]] #pour isoler le titre# pour toutes les lignes : combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})# pour supprimer les espaces inutiles : combi$Title <- sub(' ', '', combi$Title)# on peut aussi utiliser gsub pour supprimer tous les espaces mais alors " the Countess" serait devenue "theCountess" table(combi$Title)combi$Title[combi$Title %in% c('Mme', 'Mlle')] <- 'Mlle'combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'# pour remettre des factors : combi$Title <- factor(combi$Title)combi$FamilySize <- combi$SibSp + combi$Parch + 1#on prend le nom de famille mnt (qu'on va regrouper après avec le nb de personnes par famille)combi$Surname <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})# We use the function  paste  to bring two strings together, and tell it to separate them with nothing through the sep argument. This is stored to a new column called FamilyID.combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep="")combi$FamilyID[combi$FamilySize <= 2] <- 'Small'# pour voir toutes les combinaisons : famIDs <- data.frame(table(combi$FamilyID))# on le transforme pour que ne reste que les famId <= 2famIDs <- famIDs[famIDs$Freq <= 2,]# on attribue aussi small à ces cas-là puis on remet en factor : combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'combi$FamilyID <- factor(combi$FamilyID)# on les resépare en 2 et mnt ok car # If you create the above factors on the isolated test and train sets separately, there is no guarantee that both groups exist in both sets.# MAIS ICI Because we built the factors on a single dataframe, and then split it apart after we built them, R will give all factor levels to both new dataframes, even if the factor doesn’t exist in one. It will still have the factor level, but no actual observations of it in the set. train <- combi[1:891,]test <- combi[892:1309,]#Here above, the comma after that, with no numbers following, it indicates that we want to take ALL columns with this subset and store it to the assigned dataframe.# Et on relance l'arbre : rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID, data=train, method="class")# Problem of decision tree : The bias towards many-levelled factors won’t go away either, and the overfitting problem can be difficult to gauge without actually sending in submissions, but good judgement can help.# RANDOM FORESTS : summary(combi$Age)Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,                  data=combi[!is.na(combi$Age),],                   method="anova")combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])summary(combi)summary(combi$Embarked)# On les remplace par la majorité which(combi$Embarked == '')combi$Embarked[c(62,830)] = "S"# On transforme en factor : combi$Embarked <- factor(combi$Embarked)summary(combi$Fare)which(is.na(combi$Fare))combi$Fare[1044] <- median(combi$Fare, na.rm=TRUE)# Random Forests in R can only digest factors with up to 32 levels. combi$FamilyID2 <- combi$FamilyIDcombi$FamilyID2 <- as.character(combi$FamilyID2)combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'combi$FamilyID2 <- factor(combi$FamilyID2)train <- combi[1:891,]test <- combi[892:1309,]install.packages('randomForest')library(randomForest)# set the random seed in R before you begin. This makes your results reproducible next time you load the code up, otherwise you can get different classifications for each run. The number inside isn’t important, you just need to ensure you use the same seed number each time so that the same random numbers are generated inside the Random Forest functionset.seed(415)fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +                      Embarked + Title + FamilySize + FamilyID2,                    data=train,                     importance=TRUE,                     ntree=2000)#Instead of specifying  method="class"  as with  rpart , we force the model to predict our classification by temporarily changing our target variable to a factor with only two levels using  as.factor() #The  importance=TRUE  argument allows us to inspect variable importancevarImpPlot(fit)#The accuracy one tests to see how worse the model performs without each variable, so a high decrease in accuracy would be expected for very predictive variables. The Gini one digs into the mathematics behind decision trees, but essentially measures how pure the nodes are at the end of the tree. Again it tests to see the result if each variable is taken out and a high score means the variable was important.Prediction <- predict(fit, test)submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)write.csv(submit, file = "firstforest.csv", row.names = FALSE)install.packages('party')library(party)set.seed(415)# You can also override the default number of variables to choose from with  mtry , but the default is the square root of the total number available and that should work just fine. fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +                   Embarked + Title + FamilySize + FamilyID,                 data = train,                  controls=cforest_unbiased(ntree=2000, mtry=3))Prediction <- predict(fit, test, OOB=TRUE, type = "response")submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)write.csv(submit, file = "secondforest.csv", row.names = FALSE)
